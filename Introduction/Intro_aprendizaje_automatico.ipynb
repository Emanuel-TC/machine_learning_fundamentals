{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGBEbHGZAZcz",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THXyZsotAZdE",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUg2TrcjAZdR",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def genera_ejemplo_p():\n",
    "    x = np.array([10, 15, 20])\n",
    "    y = np.array([5,9,8])\n",
    "    xt = 25\n",
    "    return x, y, xt\n",
    "\n",
    "def genera_ejemplo_g():\n",
    "    x = np.array([10, 15, 20, 8, 17, 18])\n",
    "    y = np.array([5, 9, 8, 12, 6, 3])\n",
    "    xt = 25\n",
    "    return x, y, xt\n",
    "\n",
    "def pinta_ejemplo(x, y, xt, aa):\n",
    "    n = x.shape[0]\n",
    "    aa.scatter(x,y, s=60, label='datos')\n",
    "    aa.scatter(x,np.zeros(n),marker='x',s=40)\n",
    "    for ii in range(n):\n",
    "        aa.plot(np.array([x[ii],x[ii]]),\n",
    "                np.array([0, y[ii]]),\n",
    "                linestyle='--',color='blue')\n",
    "    aa.grid()\n",
    "    aa.scatter(xt,0,marker='s',color='green',s=60, label='$x_t$')\n",
    "    aa.set_xlabel('Observaciones')\n",
    "    aa.set_ylabel('target')\n",
    "    _ = aa.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVqSOAcLAZdi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción al aprendizaje Automático\n",
    "\n",
    "### Fundamentos de Aprendizaje Automático \n",
    "\n",
    "#### Enero 2025\n",
    "\n",
    "**Emilio Parrado Hernández, Vanessa Gómez Verdejo, Pablo Martínez Olmos**\n",
    "\n",
    "Departamento de Teoría de la Señal y Comunicaciones\n",
    "\n",
    "**Universidad Carlos III de Madrid**\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWnD4sx4AZdl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contenidos\n",
    "\n",
    "- Introducción al aprendizaje automático\n",
    "- Relación entre aprendizaje automático e inteligencia artificial y otras disciplinas\n",
    "- Diferencias entre algoritmos y modelos\n",
    "- Capacidades predictivas y descriptivas\n",
    "- Tipos de aprendizaje automático\n",
    "- Repaso de notebooks y python\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iosSGVH5AZdm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contexto del aprendizaje automático\n",
    "\n",
    "<img src='./img/datasicence.png' width=600 />\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9oez9g6AZdo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definiciones de aprendizaje automático\n",
    "\n",
    "El aprendizaje automático se ocupa del estudio de programas informáticos capaces de **aprender** de modo autónomo **a partir de colecciones de datos**.\n",
    "\n",
    "## Aprender \n",
    "1. Partimos de un **modelo**: **programa** que tiene un conjunto de parámetros libres \n",
    " \n",
    "2. Proceso de **optimización** mediante el cuál un **algoritmo** se encarga de asignar a estos parámetros libres unos valores que hagan que el modelo en cierta medida **explique los datos**\n",
    "\n",
    "3. El modelo ya aprendido puede **explotarse** para hacer **predicciones** acerca de datos de test (no usados para entenar)\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "Supongamos una zapatería *futurista* donde se va a instalar el siguiente módulo de **inteligencia artificial** para ayudar a los dependientes:\n",
    "\n",
    "1. Sensor en la puerta que mide la altura de cada cliente que entre en la tienda\n",
    "2. *Software* que a partir de la altura **estima** el tamaño del pie de ese cliente\n",
    "3. *Software* que carga en los dispositivos móviles de los dependientes (tablet, smartphone, etc) la estimación de tamaño de pie para que sea más fácil atender a cada cliente.\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo, obtención de datos\n",
    "\n",
    "<img src='./img/height_foot_1.png' width=400 />\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: modelo\n",
    "\n",
    "El modelo es una función matemática que nos permite hacer estimaciones del tamaño del pie a partir de la altura:\n",
    "\n",
    "$$\n",
    "\\mbox{longitud del pie en cm.} = f(\\mbox{altura en cm.})\n",
    "$$\n",
    "\n",
    "<img src='./img/height_foot_2.png' width=400 />\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: alternativas al modelo básico\n",
    "\n",
    "\n",
    "<img src='./img/height_foot_2.png' width=200 /> | <img src='./img/height_foot_3.png' width=200 />\n",
    ":---------------------:|:---------------------:\n",
    "<img src='./img/height_foot_4.png' width=200 /> | <img src='./img/height_foot_5.png' width=200 />\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZuuKmYz2AZdq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definiciones alternativas\n",
    "- Estadística con esteroides\n",
    "- Elegir un modelo y optimizarlo con *early stopping*\n",
    "- *Garbage in, garbage out*\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GvB-QPXAZdw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algunas aplicaciones del aprendizaje automático\n",
    "<img src='./img/aplicaciones_ml.png' width=800 />\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic74rVz-AZdx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Paradigmas de aprendizaje automático\n",
    "\n",
    "- Aprendizaje supervisado\n",
    "- Aprendizaje no supervisado\n",
    "- Aprendizaje por refuerzo\n",
    "- Aprendizaje incremental vs. *batch*\n",
    "- Aprendizaje adaptativo\n",
    "- Aprendizaje activo\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5azouzPAZd0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje supervisado\n",
    "\n",
    "- Observaciones + valor deseado (*target*) para cada observación\n",
    "- Aprende una relación 1 a 1 entre observaciones y *targets*.\n",
    "- Se hace una predicción por cada *target*\n",
    "- Ejemplos: *random forest*, *SVM*, procesos Gaussianos, regresores lineales\n",
    "- Aplicaciones: cualquier problema que se reduzca a aprender una **función** $y=f(\\mathbf x)$\n",
    " - *scores* para recomendaciones de *netflix*, *spotify*, etc\n",
    " - sistemas de concesiones de créditos, seguros, etc\n",
    " - reconocimiento de caras, huellas, etc\n",
    " - filtrado de correos electrónicos, noticias, etc\n",
    " - sistemas de ayuda a la diagnosis en aplicaciones clínicas\n",
    " - OCR\n",
    "\n",
    " <img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4jLBIVRAZd2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje  no supervisado\n",
    "\n",
    "- No se dispone de un *target* para cada observación\n",
    "- Detectar o descubrir **patrones** tales como grupos en las observaciones.\n",
    "- Se hace una predicción por cada *observación*\n",
    "- Ejemplos típicos: *K means*, *PCA*, *spectral clustering*, *CCA*\n",
    "- Aplicaciones:\n",
    " - agrupar colecciones de datos\n",
    " - limpieza de *outliers*\n",
    " - segmentar vídeo o audio\n",
    " - segmentación de clientes\n",
    " - organización de colecciones de documentos\n",
    " - aprendizaje de funciones de densidad de probabilidad\n",
    "\n",
    " <img src='./img/logo_uc3m_foot.jpg' width=400 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJSCqNcLAZd3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo\n",
    "- Los datos están formados por **secuencias** de observaciones/decisiones que desembocan en una **recompensa**\n",
    "- Se aprende una **estrategia** para encadenar una secuencia de decisiones que maximicen la recompensa global\n",
    "- Aplicaciones:\n",
    " - Conducción autónoma\n",
    " - Robótica\n",
    " - Videojuegos y juegos de mesa\n",
    " - diseño de estrategias de *trading*\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltjRmDd8AZd5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje  incremental vs. *batch*\n",
    "\n",
    "- Condicionado por el problema de optimización que haya que resolver\n",
    "- **Batch** se dispone de una vez del conjunto completo de datos de entrenamiento. \n",
    " - Generalmente es el caso cuando buscamos una optimización que nos dé un modelo globalmente óptimo\n",
    "- **Incremental** no se dispone del conjunto de datos de entrenamiento completo\n",
    " - Estrategia a emplear cuando el procesador en el que se ejecuta el algoritmo de entrenamiento no puede con todos los datos a la vez\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUc2LCQmAZd6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje adaptativo\n",
    "\n",
    "- Los datos de entrenamiento/test llegan de uno en uno. \n",
    "- Para cada dato que llega \n",
    " - Primero se hace una predicción\n",
    " - A continuación se hace una actualización del modelo teniendo en cuenta el error cometido en la última predicción\n",
    "- Es un *aprendizaje continuo* que necesita establecer un compromiso entre **recordar/olvidar** datos pasados\n",
    "- En escenarios donde la estadística de los datos es **no estacionaria** se pueden seguir dos estrategias principales:\n",
    " - **reentrenar** el modelo con datos *frescos* cada cierto tiempo\n",
    " - usar un algoritmo de aprendizaje **adaptativo** que vaya modificando los parámetros libres del modelo para adecuarse a las variaciones en los datos.\n",
    "\n",
    " <img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WUNaPZPAZd7",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje activo\n",
    "\n",
    "- Ideal en aplicaciones donde el coste de etiquetar los datos sea elevado\n",
    "- El algoritmo de entrenamiento tiene acceso a un conjunto de datos **sin etiquetar** y a un **oráculo** capaz de proporcionar etiquetas para esos datos.\n",
    "- Modo de **aprendizaje incremental** donde después de cada iteración el algoritmo de entrenamiento **elige** cuál de los datos no etiquetados va a ser procesado en la siguiente iteración y pide una etiqueta para ese dato al oráculo.\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFaGKdUrAZd9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problemas que se resuelven con aprendizaje automático\n",
    "\n",
    "Paso clave: traducir el problema de negocio a uno de estos *problemas tipo*\n",
    "\n",
    "- Con supervisión:\n",
    " - Clasificación\n",
    " - Regresión\n",
    " - Ranking\n",
    "- Sin supervisión\n",
    " - Agrupamiento\n",
    " - Estimación de una densidad de probabilidad\n",
    " - Detección de novedad\n",
    "- Transformaciones de los datos\n",
    "- Aprendizaje por refuerzo\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HtQsbX-AZd-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tipos/filosofías de modelado\n",
    "\n",
    "## Paramétrico vs. no paramétrico\n",
    "- **Paramétrico**: el número de parámetros libres del modelo no depende del conjunto de entrenamiento\n",
    " - Regresores lineales, clasificador de regresión logística, agrupamiento *k medias*\n",
    "- **No paramétrico**: el número de parámetros libres depende del conjunto de entrenamiento\n",
    " - k vecinos más próximos, *Support Vector Machine (SVM)*\n",
    " \n",
    "Pero existe una escala de grises entre métodos claramente paramétricos y métodos claramente no paramétricos : *random forest*, *modelos de mezclas de Gaussianas*\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ll4HqebAZd_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos generativos vs. discriminativos\n",
    "- **Generativos**: Estimar función de densidad de probabilidad (fdp) que genera los datos y buscar la **solución óptima** para el problema **completamente caracterizado**.\n",
    " - *Linear Discriminant Analysis*\n",
    "- **Discriminativos**: usar los datos para aprender una función $y=f(\\mathbf x)$ que resuelva el problema para los **datos de entrenamiento**\n",
    " - SVM\n",
    "\n",
    " <img src='./img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-S0g-F7AZeB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo de modelo paramétrico vs. no paramétrico\n",
    "\n",
    "Planteamos un problema (*toy problem*) de regresión en 1D. Tenemos 3 observaciones en 1 dimensión (cruces naranjas en la figura), los targets son los círculos azules y el objetivo es encontrar el target para la observación que está en el cuadrado verde, que hemos llamado $x_t$.\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1702,
     "status": "ok",
     "timestamp": 1592304260965,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "RpJmo_9ZAZeE",
    "outputId": "c87e13aa-aed9-400c-8928-ea62a15e79f5"
   },
   "outputs": [],
   "source": [
    "x,y,xt = genera_ejemplo_p()\n",
    "ff,aa = plt.subplots(1,1)\n",
    "pinta_ejemplo(x,y,xt, aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JqO0SBwZkgk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo paramétrico\n",
    "Vamos a resolver el problema primero aplicando un método **paramétrico** de regresión: la **regresión lineal**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYMtdoDQAZeC",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Este modelo, conocido por todos, depende de dos parámetros:\n",
    "- la pendiente $a$ y \n",
    "- el término de sesgo $b$.\n",
    "\n",
    "La función que relaciona observaciones con targets viene dada por \n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "donde $x$ son las observaciones e $y$ los targets correspondientes a esas observaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1EZNWGWAZeO",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El **algoritmo** de entrenamiento debe encontrar valores para $a$ y $b$ tales que expliquen los datos, es decir, que el error que se cometa al estimar los targets $y$ con el **modelo** $ax+b$ sea mínimo:\n",
    "$$\n",
    "\\mbox{error } = (y - (ax+b))^2\n",
    "$$\n",
    "\n",
    "Para no hacer *spoliers* de la clase de regresión vamos a emplear un algoritmo de optimización bastante rudimentario:\n",
    "1. Elegir 5 valores al azar para la pareja de parámetros $(a,b)$. Son los candidatos a modelo\n",
    "2. Evaluar el error que cometemos al intentar modelar las observaciones con cada uno de los candidatos a modelo\n",
    "3. Determinar como modelo final el de menor error y usarlo para estimar el target $y_t$ correspondiente a $x_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2112,
     "status": "ok",
     "timestamp": 1592304261382,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "OR_jQLsYAZeU",
    "outputId": "c3321581-41e9-4e19-b0a5-5c409dc658e2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "# Elegir candidatos\n",
    "n_candidatos =5\n",
    "B = np.round(np.random.uniform(-1,10,size=n_candidatos),2)\n",
    "A = np.round(np.random.uniform(-.5,.5,size=n_candidatos),2)\n",
    "\n",
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "\n",
    "xg = np.array([8,30])\n",
    "for ii in range(n_candidatos):\n",
    "    yg = A[ii] * xg  + B[ii]\n",
    "    aa.plot(xg, yg, linestyle=':', linewidth=2, label=\"c{0:d}\".format(ii))\n",
    "\n",
    "_=aa.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2107,
     "status": "ok",
     "timestamp": 1592304261384,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "P8-g-n4PAZed",
    "outputId": "42ebee3a-2435-4728-ac9c-968fe8f06232",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el error de cada candidato\n",
    "E = np.empty(n_candidatos)\n",
    "for ii in range(n_candidatos):\n",
    "    pred = A[ii] * x + B[ii]\n",
    "    E[ii] = np.sum((y - pred)**2)\n",
    "    print(\"Candidato c{0:d}, error: {1:.2f}\".format(ii,E[ii]))\n",
    "# Mejor candidato\n",
    "best_c = np.argmin(E)\n",
    "print(\"------------------------\")\n",
    "print(\"El mejor candidato es c{0:d} \".format(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1592304262011,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "ZGHfZ6RJAZek",
    "outputId": "523f3708-f243-4aed-f840-433ce5222a5a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "\n",
    "xg = np.array([8,30])\n",
    "for ii in range(n_candidatos):\n",
    "    yg = A[ii] * xg  + B[ii]\n",
    "    aa.plot(xg, yg, linestyle=':')\n",
    "yg = A[best_c] * xg  + B[best_c]\n",
    "aa.plot(xg, yg, linewidth='2', label='mejor c')\n",
    "\n",
    "_=aa.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1592304262012,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "jwRalEOCAZes",
    "outputId": "bb51d576-1619-41e2-9c90-f83d5b7a7fb6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Valores escogidos para los parámetros: a = {0:.2f}; b = {1:.2f}\".format(A[best_c], B[best_c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vr0hQ0YUAZe3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inferencia sobre el valor de $y_t$ cuando la observación es $x_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogkVjuPjAZe4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "yt = A[best_c] * xt + B[best_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3118,
     "status": "ok",
     "timestamp": 1592304262421,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "jdSVBHVAAZe-",
    "outputId": "649a5cac-49e9-4d2e-947a-1991ce338ca1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "xg = np.array([8,30])\n",
    "yg = A[best_c] * xg  + B[best_c]\n",
    "aa.plot(xg, yg, linewidth='2', label='modelo', color='brown')\n",
    "\n",
    "aa.plot(np.array([xt,xt]),\n",
    "            np.array([0, yt]),\n",
    "            linestyle='--',color='green')\n",
    "aa.scatter(xt,yt, s=60, label='$y_t$', color='green')\n",
    "_=aa.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pdxmo2aQAZfI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decimos que el método es paramétrico porque el número de parámetros no cambia si variamos el conjunto de entrenamiento. Veamos qué ocurre si aparecen nuevos datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3112,
     "status": "ok",
     "timestamp": 1592304262422,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "cAuGVwxkAZfJ",
    "outputId": "23c0d3e4-bdfd-4b05-f0ce-62c577d2bbd1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x,y,xt = genera_ejemplo_g()\n",
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "xg = np.array([8,30])\n",
    "yg = A[best_c] * xg  + B[best_c]\n",
    "aa.plot(xg, yg, linewidth='2', label='modelo', color='brown', linestyle=':')\n",
    "\n",
    "aa.plot(np.array([xt,xt]),\n",
    "            np.array([0, yt]),\n",
    "            linestyle='--',color='green')\n",
    "aa.scatter(xt,yt, s=60, label='$y_t$', color='green')\n",
    "_=aa.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6jKqeqSAZfP",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora es necesario volver a emplear el **algoritmo** para recalibrar el modelo y que explique también los nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3428,
     "status": "ok",
     "timestamp": 1592304262745,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "-rTwpZIHAZfR",
    "outputId": "9e409602-2af8-4f7a-f73a-872df11f87cc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_candidatos = 5\n",
    "B = np.round(np.random.uniform(5,15,size=n_candidatos),2)\n",
    "A = np.round(np.random.uniform(-.5,.5,size=n_candidatos),2)\n",
    "\n",
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "\n",
    "xg = np.array([6,30])\n",
    "for ii in range(n_candidatos):\n",
    "    yg = A[ii] * xg  + B[ii]\n",
    "    aa.plot(xg, yg, linestyle=':', linewidth=2, label=\"c{0:d}\".format(ii))\n",
    "\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3422,
     "status": "ok",
     "timestamp": 1592304262746,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "hidlHhLCAZfi",
    "outputId": "e1fb6ca6-69ee-4c11-dfff-726987b19f61",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el error de cada candidato\n",
    "E = np.empty(n_candidatos)\n",
    "for ii in range(n_candidatos):\n",
    "    pred = A[ii] * x + B[ii]\n",
    "    E[ii] = np.sum((y - pred)**2)\n",
    "    print(\"Candidato c{0:d}, error: {1:.2f}\".format(ii,E[ii]))\n",
    "# Mejor candidato\n",
    "best_c = np.argmin(E)\n",
    "print(\"------------------------\")\n",
    "print(\"El mejor candidato es c{0:d} \".format(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3873,
     "status": "ok",
     "timestamp": 1592304263204,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "ekCPR5bmAZfo",
    "outputId": "b098e225-9864-4679-b38e-446a2880548c",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "xg = np.array([8,30])\n",
    "for ii in range(n_candidatos):\n",
    "    yg = A[ii] * xg  + B[ii]\n",
    "    aa.plot(xg, yg, linestyle=':')\n",
    "yg = A[best_c] * xg  + B[best_c]\n",
    "aa.plot(xg, yg, linewidth='2', label='mejor c')\n",
    "\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3865,
     "status": "ok",
     "timestamp": 1592304263204,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "EPRRD9uuAZft",
    "outputId": "96dd0081-76ea-4dda-aef2-07f22f73d0cb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Valores escogidos para los parámetros: a = {0:.2f}; b = {1:.2f}\".format(A[best_c], B[best_c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFE03lySAZfz",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "yt = A[best_c] * xt + B[best_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4310,
     "status": "ok",
     "timestamp": 1592304263661,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "APIXyednAZf5",
    "outputId": "d68a598a-743c-4fc0-a075-8388e28ce849",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "\n",
    "xg = np.array([8,30])\n",
    "yg = A[best_c] * xg  + B[best_c]\n",
    "plt.plot(xg, yg, linewidth='2', label='modelo', color='brown')\n",
    "\n",
    "plt.plot(np.array([xt,xt]),\n",
    "            np.array([0, yt]),\n",
    "            linestyle='--',color='green')\n",
    "plt.scatter(xt,yt, s=60, label='$y_t$', color='green')\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8xBClBSAZf_",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El algoritmo puede que necesite más tiempo para aprender el modelo, pero el número de parámetros no cambia, y por tanto el **tiempo que se tarde en hacer la inferencia para $x_t$ tampoco cambia**.\n",
    "\n",
    "Hemos elegido el modelo $y=ax+b$ con conocimiento *a priori*, antes de ver los datos. Si nuestra intuición a priorística fuese diferente, p. ej. dependencia como un polinomio de orden 2, pues tendríamos que cambiar el modelo y aprender otro juego diferente de parametros con los datos, por ejemplo $y=ax^2+bx+c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5NPa_EIAZgA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo no paramétrico\n",
    "\n",
    "**El vecino más próximo** (*nearest neighbour*) es posiblemente uno de los métodos menos paramétricos que podemos encontrarnos. El modelo no tiene **ningún parámetro**, y lo único que asume es que si dos observaciones son muy parecidas (son vecinas) sus correspondientes *targets* también van a ser parecidos.\n",
    "\n",
    "Por tanto, el **modelo** de un regresor basado en el vecino más próximo consiste en una tabla en la que se guardan todas las observaciones del conjunto de entrenamiento y sus correspondientes *targets*. La **predicción** para la muestra de test $x_t$ será el *target* correspondiente a la observación del conjunto de entrenamiento $x_*$ más próxima a $x_t$. Matemáticamente\n",
    "$$\n",
    "y_t = y_j \\mbox{ tal que } j = \\mbox{argmin}_{i} |x_i - x_t|\n",
    "$$\n",
    "\n",
    "\n",
    "Vamos a verlo en el ejemplo con el que estamos trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4643,
     "status": "ok",
     "timestamp": 1592304264001,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "GSwafROwAZgC",
    "outputId": "7e2ef1db-c251-4b86-ca4c-d9c8ff66a210",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x,y,xt = genera_ejemplo_p()\n",
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4636,
     "status": "ok",
     "timestamp": 1592304264001,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "o9kfVz9dAZgH",
    "outputId": "53cd8824-7a3b-450b-e3a2-5e620a3fe4f1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = x.shape[0]\n",
    "distancia = np.empty(n)\n",
    "for ii,ix in enumerate(x):\n",
    "    distancia[ii] = np.absolute(xt-ix)\n",
    "    print(\"distancia x_{0:d} a x_t: {1:.2f}\".format(ii, distancia[ii]))\n",
    "vecino = np.argmin(distancia)\n",
    "print(\"-----------------------------\")\n",
    "print(\"El vecino más próximo es x_{0:d}\".format(vecino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o91pPVIBAZgL",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "yt = y[vecino]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4626,
     "status": "ok",
     "timestamp": 1592304264002,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "koVmScI-AZgS",
    "outputId": "32d6163f-633d-4371-8df6-5d726743a5b1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "\n",
    "\n",
    "plt.plot(np.array([xt,xt]),\n",
    "            np.array([0, yt]),\n",
    "            linestyle='--',color='green')\n",
    "plt.scatter(xt,yt, s=60, label='$y_t$', color='green')\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhaNCEMKAZgX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si aparecen más datos de entrenamiento, el modelo aumenta porque estos datos tienen que incorporarse a la tabla mediante la cuál se calculan las predicciones. Cuando añadimos nuevos datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4905,
     "status": "ok",
     "timestamp": 1592304264288,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "84UfDOXaAZgY",
    "outputId": "c5890d3a-f1ca-4b19-96e9-67173092973a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x,y,xt = genera_ejemplo_g()\n",
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4899,
     "status": "ok",
     "timestamp": 1592304264289,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "95El3LFqAZgd",
    "outputId": "f98b3abc-36e7-4b70-b2fc-99cb4a1d266a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = x.shape[0]\n",
    "distancia = np.empty(n)\n",
    "for ii,ix in enumerate(x):\n",
    "    distancia[ii] = np.absolute(xt-ix)\n",
    "    print(\"distancia x_{0:d} a x_t: {1:.2f}\".format(ii, distancia[ii]))\n",
    "vecino = np.argmin(distancia)\n",
    "print(\"-----------------------------\")\n",
    "print(\"El vecino más próximo es x_{0:d}\".format(vecino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huwMz4mIAZgh",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "yt = y[vecino]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5604,
     "status": "ok",
     "timestamp": 1592304265005,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "LjfkW5G_AZgl",
    "outputId": "5db971a2-a1ba-46f3-bbcd-273d986acf38",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1, figsize=(8,6))\n",
    "pinta_ejemplo(x,y,xt,aa)\n",
    "\n",
    "\n",
    "plt.plot(np.array([xt,xt]),\n",
    "            np.array([0, yt]),\n",
    "            linestyle='--',color='green')\n",
    "plt.scatter(xt,yt, s=60, label='$y_t$', color='green')\n",
    "_=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91XEF08iAZgq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora el tiempo para calcular $y_t$ aumenta porque hay que evaluar un número mayor de posibles vecinos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zmwrds-AAZgr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ¿Y cuál es la mejor aproximación?\n",
    "Pues depende. Cada una de estas aproximaciones tiene sus ventajas y sus inconvenientes. Una de las **claves** fundamentales en el éxito de un método de aprendizaje automático es resolver el **compromiso** entre:\n",
    "- **conocimiento a priori**: todo lo que sabemos teóricamente acerca del problema **antes de acceder a los datos**\n",
    "- **conocimiento a posteriori**: todo lo que aprendemos mediante el **análisis de los datos**.\n",
    "\n",
    "Los métodos paramétricos permiten introducir conocimiento a priori en el modelo. En el caso que estábamos estudiando ese conocimiento es que el *target* es **proporcional a la magnitud de la observación**. Si esta asunción es verdad, el método paramétrico arregla bastante la situación, pero ¿y si no es verdad?\n",
    "\n",
    "Los métodos **no paramétricos** nos permiten ser más **agnósticos** acerca de la estructura final del modelo. Pero son más **demandantes de datos** (para compensar esa ausencia de conocimiento teórico a priori).\n",
    "\n",
    "Además, en general, los métodos paramétricos conducen a optimizaciones más sencillas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcldNVIxAZgr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo Generativo frente a Discriminativo\n",
    "\n",
    "Recordamos: en esencia un modelo **generativo** pasa por un primer paso de aprender el **proceso estadístico** mediante el cuál se generan los datos. En un problema de clasificación esto equivale a aprender la **función de densidad de probabilidad** que genera las observaciones de cada clase, y la **probabilidad a priori** de cada clase.\n",
    "\n",
    "Planteamos ahora un *toy problem* de clasificación con dos clases en 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6267,
     "status": "ok",
     "timestamp": 1592304265674,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "hDvEYYAkAZgs",
    "outputId": "14f50248-dffa-4221-9b88-59ce1a57b6a4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.array([[-2.1 ,  0.08],\n",
    "       [-1.16,  0.01],\n",
    "       [-0.75,  0.37],\n",
    "       [-1.34,  0.45],\n",
    "       [-0.16, -0.38],\n",
    "       [ 1.43,  0.54],\n",
    "       [ 0.85, -0.43],\n",
    "       [ 0.03,  0.33]])\n",
    "X2 = np.array([[ 2.89, -0.02],\n",
    "       [ 3.56,  0.02],\n",
    "       [ 3.11, -0.24],\n",
    "       [ 2.98, -0.15]])\n",
    "n1 = X1.shape[0]\n",
    "n2 = X2.shape[0]\n",
    "X = np.vstack((X1,X2))\n",
    "y = np.hstack((np.ones(n1), -1*np.ones(n2)))\n",
    "ff,aa = plt.subplots(1,1,figsize=(8,6))\n",
    "aa.scatter(X1[:,0],X1[:,1],marker='o', color='blue')\n",
    "aa.scatter(X2[:,0],X2[:,1],marker='x', color='red')\n",
    "_ = aa.set_ylim([-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOXN9yKxAZgw",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Aprender el modelo generativo de los datos\n",
    "Vamos a asumir que cada clase se ha generado con una gaussiana con matriz de covarianzas diagonal.  \n",
    "Para la clase positiva, $y=+1$\n",
    "$$\n",
    "p(\\mathbf x | y=+1) = \\mathcal N\\left(\\left [\\begin{array}{c} x_1 \\\\ x_2 \\end{array}\\right]|\\left [\\begin{array}{c} m_{+1,1}\\\\ m_{+1,2} \\end{array}\\right], \\left [\\begin{array}{cc} s_{+1,1} & 0 \\\\ 0 & s_{+1,2} \\end{array}\\right]\\right)\n",
    "$$ \n",
    "\n",
    "Y para la clase negativa, $y=-1$\n",
    "$$\n",
    "p(\\mathbf x | y=-1) = \\mathcal N\\left(\\left [\\begin{array}{c} x_1 \\\\ x_2 \\end{array}\\right]|\\left [\\begin{array}{c} m_{-1,1}\\\\ m_{-1,2} \\end{array}\\right], \\left [\\begin{array}{cc} s_{-1,1} & 0 \\\\ 0 & s_{-1,2} \\end{array}\\right]\\right)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ist-soNjAZgx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Aplicar teoría de la decisión sobre el modelo de datos\n",
    "Si completamos el modelo de los datos con las probabilidades a priori de cada clase:\n",
    "- $\\pi_1=p(y=+1)$ \n",
    "- $\\pi_{-1}=p(y=-1)$\n",
    "\n",
    "Sabemos que el **clasificador óptimo** consiste en asignar cada muestra a la clase que tiene una mayor **probabilidad a posteriori**.\n",
    "$$\n",
    "y = j \\quad \\mbox{ tal que } \\quad j=\\mbox{argmax}_{i \\in \\{-1,1\\}} p(y=i| \\mathbf x) = \\mbox{argmax}_{i \\in \\{-1,1\\}} p(\\mathbf x|y=i) \\pi_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHErvKS8AZgy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplicamos esta estrategia en el problema planteado\n",
    "\n",
    "### 1. Aprender el modelo generativo\n",
    "\n",
    "#### 1.1.- Estimar las medias de cada clase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6261,
     "status": "ok",
     "timestamp": 1592304265675,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "uTbvD-G5AZgy",
    "outputId": "0e3a04e9-da9b-4492-fa93-7ff814d26ff6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m1_ = np.mean(X[y==1,:],0)\n",
    "print(\"media de la clase 1:\")\n",
    "print(np.round(m1_,2))\n",
    "print(\"\")\n",
    "m2_ = np.mean(X[y==-1,:],0)\n",
    "print(\"media de la clase -1:\")\n",
    "print(np.round(m2_,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z30yqEtAZg4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.2.- Estimar las covarianzas de cada clase\n",
    "Como hemos fijado que las covarianzas son diagonales, solo tenemos que estimar los términos de las diagonales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6253,
     "status": "ok",
     "timestamp": 1592304265675,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "GQV1WHgzAZg5",
    "outputId": "fddb1082-d543-484f-9780-43f5f28fb5e5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "S1_ = np.diag(np.round(np.var(X[y==1,:],0),2))\n",
    "print(\"Covarianza de la clase 1:\")\n",
    "print(S1_)\n",
    "print(\"\")\n",
    "S2_ = np.diag(np.round(np.var(X[y==-1,:],0),2))\n",
    "print(\"Covarianza de la clase -1:\")\n",
    "print(S2_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ezlMxMwAZg9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.3.- Estimar las probabilidades a priori\n",
    "Estimaciones mediante frecuencias relativas de cada clase en el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6247,
     "status": "ok",
     "timestamp": 1592304265676,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "1uVWFVggAZg9",
    "outputId": "cfa07492-2cc6-4e11-dd33-3075f04ae49f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pi1_ = np.mean(y==1)\n",
    "pi2_ = np.mean(y==-1)\n",
    "print(\"Probabilidad de la clase 1: {0:.2f}\".format(pi1_))\n",
    "print(\"Probabilidad de la clase -1: {0:.2f}\".format(pi2_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9cCIZqKAZhB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.- Aplicar teoría de la decisión\n",
    "#### 2.1.- Construir del decisor óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSwfMYwYAZhC",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class decisor_optimo(object):\n",
    "    def __init__(self, m1, m2, S1, S2, pi1, pi2, y1, y2):\n",
    "        # Gaussiana de la clase 1\n",
    "        self.G1 = multivariate_normal(mean = m1, \n",
    "                                      cov = S1)\n",
    "        self.pi1 = pi1 # prob. a priori de la clase 1\n",
    "        # Gaussiana de la clase 2\n",
    "        self.G2 = multivariate_normal(mean = m2, \n",
    "                                      cov = S2)\n",
    "        self.pi2 = pi2 # prob. a priori de la clase 2\n",
    "        self.y1 = y1 # etiqueta de la clase 1\n",
    "        self.y2 = y2 # etiqueta de la clase 2\n",
    "    def predict(self, x):\n",
    "        py1 = self.G1.pdf(x)*self.pi1  # posterior clase 1\n",
    "        py2 = self.G2.pdf(x)*self.pi2 # posterior clase 2\n",
    "        n = x.shape[0]\n",
    "        if n > 1:\n",
    "            output = np.array([self.y1] * n)\n",
    "            output[py2 > py1] = self.y2\n",
    "        else:\n",
    "            output = self.y1\n",
    "            if py2 > py1:\n",
    "                output = self.y2\n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dKPNTqHAZhF",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2.2.- Explotar el decisor óptimo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6235,
     "status": "ok",
     "timestamp": 1592304265676,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "7CHx73KFAZhF",
    "outputId": "b45c6cad-ee7c-4c40-d477-2d5d290390b8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1,figsize=(8,6))\n",
    "aa.scatter(X1[:,0],X1[:,1],marker='o', color='blue')\n",
    "aa.scatter(X2[:,0],X2[:,1],marker='x', color='red')\n",
    "\n",
    "clf = decisor_optimo(m1_, m2_, S1_, S2_, pi1_, pi2_, 1, -1)\n",
    "plot_step=0.05\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = aa.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.2)\n",
    "_ = aa.set_ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0jHblvNAZhK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Aproximación basada en modelo discriminativo\n",
    "\n",
    "En general conseguir un **modelo generativo** de los datos suele acabar en un problema de optimización varios órdenes de magnitud más complejo que encontrar un clasificador que separe bien las clases. Pensemos por ejemplo en estas dos situaciones:\n",
    "- Clasificador para separar fotos de camiones de fotos de motos\n",
    "- Clasificador para separar fotos de camiones con averías de fotos de camiones intactos.\n",
    "\n",
    "Si nuestro ejemplo responde más bien a la primera situación, no hace falta caracterizar perfectamente todos los detalles de un camión o de una moto para separar, basta con fijarnos en las **características suficientes** para separar con bastante confianza.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el caso que nos ocupa, podríamos intentar separar las dos clases con un test de umbral en una de las dos variables. Este clasificador se llama *stump* y es la base de los árboles de decisión que vamos a ver en la siguiente sesión. El modelo *stump* se define con 3 parámetros:\n",
    "- variable sobre la que se realiza el test\n",
    "- valor del umbral\n",
    "- sentido de la clasificación (qué clase se asigna a cada lado)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Siguiendo con la política de *no spoiler* vamos a usar un **algoritmo** poco inteligente para entrenar el **modelo stump**. \n",
    "\n",
    "### Algoritmo para entrenar el *stump*\n",
    "\n",
    "1. Elegir al azar 4 umbrales para cada variable\n",
    "2. Determinar la clase de salida para cada lado del umbral en función de la mayoría de ejemplos de entrenamiento que hayan caído en cada lado\n",
    "3. Evaluar el error de clasificación que comete cada *stump*\n",
    "4. Elegir como clasificador final el mejor *stump*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yATxXxlWAZhK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_stumps = 4\n",
    "umbrales_1 = np.round(np.random.uniform(-2,3.5,size=n_stumps),2)\n",
    "umbrales_2 = np.round(np.random.uniform(-0.4,0.5,size=n_stumps),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8054,
     "status": "ok",
     "timestamp": 1592304267506,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "KIPZ4oHVAZhN",
    "outputId": "aa065a4a-d62f-4869-eb16-5eeb084ee6a3"
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(n_stumps,2, sharex=True, sharey=True, figsize=(8,10))\n",
    "for ss in range(n_stumps):\n",
    "    aa[ss][0].scatter(X1[:,0],X1[:,1],marker='o', color='blue')\n",
    "    aa[ss][0].scatter(X2[:,0],X2[:,1],marker='x', color='red')\n",
    "    aa[ss][0].plot(umbrales_1[ss]*np.ones(2),\n",
    "                  np.array([-1,1]), color='black')\n",
    "    aa[ss][1].scatter(X1[:,0],X1[:,1],marker='o', color='blue')\n",
    "    aa[ss][1].scatter(X2[:,0],X2[:,1],marker='x', color='red')\n",
    "    aa[ss][1].plot(np.array([-3,4]), \n",
    "                   umbrales_2[ss]*np.ones(2),\n",
    "                   color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy7LqYuqAZhT"
   },
   "outputs": [],
   "source": [
    "class stump(object):\n",
    "    def __init__(self, v, u, x, y):\n",
    "        self.v = v\n",
    "        self.u = u\n",
    "        cuales_izq = np.where(x[:,v] <= u)[0]\n",
    "        if np.mean(y[cuales_izq]) < 0:\n",
    "            self.y_izq = -1\n",
    "        else:\n",
    "            self.y_izq = 1\n",
    "        cuales_dcha = np.where(x[:,v] > u)[0]\n",
    "        if np.mean(y[cuales_dcha]) < 0:\n",
    "            self.y_dch = -1\n",
    "        else:\n",
    "            self.y_dch = 1\n",
    "    def predict(self, x):\n",
    "        n = len(x)\n",
    "        output = self.y_dch * np.ones(n)\n",
    "        cuales_izq = np.where(x[:,self.v] <= self.u)[0]\n",
    "        output[cuales_izq] = self.y_izq\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aw0XRYhAZhY"
   },
   "outputs": [],
   "source": [
    "lista_stumps = []\n",
    "for ss in range(n_stumps):\n",
    "    lista_stumps.append(stump(v=0, u=umbrales_1[ss], x=X, y=y))\n",
    "    lista_stumps.append(stump(v=1, u=umbrales_2[ss], x=X, y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8040,
     "status": "ok",
     "timestamp": 1592304267508,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "5gPE-HZzAZhb",
    "outputId": "d1cc2617-c122-4ba4-b21e-b97978c80c49"
   },
   "outputs": [],
   "source": [
    "aciertos = np.empty(len(lista_stumps))\n",
    "for iis,ss in enumerate(lista_stumps):\n",
    "    pred = ss.predict(X)\n",
    "    aciertos[iis] = np.mean(pred==y)\n",
    "    print(\"Stump {0:d}, if x[{1:d}] <= {2:.2f} then y={3:d} --> acierto: {4:.1f}%\".format(iis,\n",
    "                                                                                          ss.v,\n",
    "                                                                                          ss.u,\n",
    "                                                                                          ss.y_izq,\n",
    "                                                                                          aciertos[iis]*100. ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8033,
     "status": "ok",
     "timestamp": 1592304267508,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "Olv6qGTlAZhg",
    "outputId": "1d07a796-5437-40e7-e7b1-cba7fa2cae7c"
   },
   "outputs": [],
   "source": [
    "best_s = np.argmax(aciertos)\n",
    "print(\"El mejor clasificador es Stump {0:d}\".format(best_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8028,
     "status": "ok",
     "timestamp": 1592304267509,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "J8_5qoZbAZhk",
    "outputId": "49dafd69-288e-4955-fa8d-357b8e54036d"
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1,figsize=(8,6))\n",
    "aa.scatter(X1[:,0],X1[:,1],marker='o', color='blue')\n",
    "aa.scatter(X2[:,0],X2[:,1],marker='x', color='red')\n",
    "\n",
    "clf = lista_stumps[best_s]\n",
    "plot_step=0.05\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = aa.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.2)\n",
    "_ = aa.set_ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VqDkp5TAZho",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Y cuál es la mejor aproximación?\n",
    "Pues depende. Cada una de estas aproximaciones tiene sus ventajas y sus inconvenientes. Los modelos generativos y discriminativos tienen maneras diferentes de introducir conocimiento a priori en el modelo. \n",
    "\n",
    "Los modelos generativos suelen ser mucho más demandantes de datos. Además hay un compromiso en el empleo de densidades de probabilidad fáciles de manejar, para que la aplicación de la teoría de decisión óptima sea analíticamente manejable.\n",
    "\n",
    "Los modelos discriminativos suelen conducir a optimizaciones más manejables, aunque habría que estudiar cada caso concreto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wImZlC7FAZhp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La disyuntiva entre modelos generativos y discriminativos refleja otro de los **compromisos** que hay que resolver en el aprendizaje automático: la disyuntiva entre\n",
    "- un **modelo correcto con parámetros erróneos**: esto es, sabemos la **forma** que debe tener el modelo generativo pero seguramente no tengamos datos suficientes para estimar correctamente todos los parámetros que necesita esa forma\n",
    "- un **modelo incorrecto con los parámetros bien optimizados**: esto es, sabemos que en el proceso de generación de los datos no interviene que la **frontera de decisión** deba tener la expresión que hemos elegido, pero tenemos suficientes datos para optimizar esa frontera para el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8yjbUFEAZhq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Familias de modelos de aprendizaje automático\n",
    "\n",
    "- **Modelos lineales**: **LDA**, **regresión logística**, **LASSO**, **ridge regression**, **PCA**, *Partial Least Squares*, *Canonical Correlation Analysis*\n",
    "- **Métods kernel**: *Support Vector Machines*, procesos gaussianos, agrupamiento espectral, kernel PCA, kernel PLS, kernel LDA\n",
    "- **Árboles**\n",
    "- **Vecinos más próximos**\n",
    "- **Modelos probabilísticos**: **K medias**, **mezclas de gaussianas**, *Latent Dirichlet Alocation*, Modelos Ocultos de Markov, redes bayesianas\n",
    "- **Redes Neuronales**: Perceptrones multicapa, *Deep Learning*\n",
    "- **Modelos de conjuntos**: **Random Forest**, *bagging*, *boosting*, mezclas de expertos\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uI5LxqnFAZhr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos lineales\n",
    "El modelo es una función lineal de las variables que componen las observaciones\n",
    "- Optimizaciones en general poco costosas\n",
    "- Relativamente fácil de explicar la contribución de cada variable a través de su peso en el modelo\n",
    "- Ejemplos:\n",
    " - Clasificación:\n",
    "   - Regresión Logística\n",
    "   - *Linear Discriminant Analysis*\n",
    "   - *Linear Support Vector Machine*\n",
    " - Regresión:\n",
    "   - LASSO\n",
    "   - *Ridge regression*\n",
    "   - *Partial Least Squares*\n",
    " - No supervisado:\n",
    "   - Análisis en Componentes Principales\n",
    "   - *Canonical Correlation Analysis*\n",
    "   \n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3NTS7JBAZhr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Métodos *kernel*\n",
    "Consiguen modelos **no lineales** mediante combinaciones lineales de funciones base no lineales.\n",
    "- Clasificación:\n",
    "    - *Kernel Linear Discriminant Analysis*\n",
    "    - *Support Vector Machine*\n",
    "- Regresión:\n",
    "    - *Support Vector Regression*\n",
    "    - Procesos Gaussianos\n",
    "    - *Kernel Partial Least Squares*\n",
    "- No supervisado:\n",
    "    - *Kernel PCA*\n",
    "    - *Kernel Canonical Correlation Analysis*\n",
    "    - *One-class SVM*\n",
    "    \n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKDnFnnmAZhs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Árboles de decisión para clasificación y  regresión\n",
    "\n",
    "Alternativa sencilla para construir modelos no lineales que pueden llegar a ser más o menos fáciles de explicar\n",
    "- Se construye una **estructura jerárquica** que parte de un nodo inicial que incluye todos los datos de entrenamiento\n",
    "- Se dividen los nodos **recursivamente** mediante un test de **umbral** en una de las variables que modo que los datos del nodo se van a dos nodos hijos que se supone están formados por datos más **homogéneos**\n",
    "- Criterios de parada para decidir cuándo un nodo se declara **hoja**, y ya no se sigue subdividiendo\n",
    "\n",
    "<img src='./img/arbol.png' width=400 />\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXL2LPxdAZhs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes neuronales\n",
    "- Posiblemente el método más **representativo** del aprendizaje automático. Siempre vuelven al **foco**\n",
    "- Son un método **general** que puede adaptarse a problemas de clasificación, regresión, detección de patrones, etc\n",
    "- **Inspiración** en las redes de neuronas naturales\n",
    "  - Conexión con **neurociencia** \n",
    "  - Una neurona es una unidad elemental de cálculo que recibe unas entradas, las combina linealmente y dispara con una función sigmoide\n",
    "\n",
    "\n",
    "  <img src='./img/neurona.png' width=300 />\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Redes neuronales incluyen neuronas conectadas en varias capas y pueden aproximar cualquier función\n",
    "  \n",
    "  \n",
    "  <img src='./img/rrnn.png' width=400 />\n",
    "\n",
    "\n",
    "- *Deep Learning*\n",
    "- *Deep Reinforcement Learning*\n",
    "\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ_2cw9mAZht",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos basados en *ensembles*\n",
    "Combinan las decisiones de **muchos aprendices razonablemente sencillos** para aprender un concepto complicado\n",
    "- **Bagging**\n",
    "  - Entrenar cada aprendiz con una subconjunto aleatorio de los datos de entrenamiento y combinar las predicciones de todos los aprendices en una votación\n",
    "- **Boosting**\n",
    "  - Entrenar a los aprendices en modo secuencial: cada aprendiz trata de corregir los errores de los anteriores\n",
    "- Ejemplo más conocido: **random forest**\n",
    "  - Combinan árboles de decisión\n",
    "  - Cada árbol aprende con un subconjunto de los datos y un subconjunto de las variables. Así se filtra ruido\n",
    "  - Se puede analizar la relevancia de cada variable \n",
    "  - **Extremely random forest**\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qgk-YhJAZht",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo intuitivo de *kernel method*\n",
    "\n",
    "Vamos a plantear un *toy problem* de regresión donde los datos demanden un modelo no lineal, y a resolverlo con una combinación lineal de funciones base no lineales (*kernels*).\n",
    "\n",
    "<img src='./img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPN3dKMYAZhv",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xr = np.array([0.5, 1, 2.5, 4, 5.5, 6, 7, 7.5, 8, 10])\n",
    "yr = np.array([0.1, 0.15, 0.09, 0.085, 0.17, 0.25, 0.18, 0.07, 0.02, 0.01 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8580,
     "status": "ok",
     "timestamp": 1592304268072,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "VRzfXHzZAZh2",
    "outputId": "4844da45-2646-4c99-a00e-912a4ab77e24",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(1,1)\n",
    "_=aa.scatter(xr,yr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWYE651UAZh8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para este problema sencillo proponemos un modelo con dos funciones base (*kernels*) que sean de tipo *Radial Basis Function*\n",
    "$$\n",
    "f(x) = \\kappa(x; c,\\gamma) = \\exp(-\\gamma (c-x)^2)\n",
    "$$ donde \n",
    "- $c$ es el centro de la función base y en este caso concreto van a ser cualquiera de las observaciones del conjunto de entrenamiento\n",
    "- $\\gamma$ mide la cobertura de las funciones base, es un parámetro que hay que aprender\n",
    "\n",
    "Ejemplos de funciones base centradas en alguna de las observaciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8572,
     "status": "ok",
     "timestamp": 1592304268072,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "wvIP6FxTAZh8",
    "outputId": "14b37696-48e3-41a0-cfea-95d5292f87ab",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(0,10,1000)\n",
    "n1 = norm(loc=xr[1],scale=0.8) \n",
    "x1=n1.pdf(x)\n",
    "n2 = norm(loc=xr[5],scale=1)\n",
    "x2 = n2.pdf(x)\n",
    "w1 = 0.8\n",
    "w2= 2.6\n",
    "plt.figure()\n",
    "plt.scatter(xr,yr)\n",
    "plt.fill_between(x,0,x1*w1,alpha=0.2)\n",
    "plt.fill_between(x,0,x2*w2,alpha=0.2)\n",
    "plt.plot([xr[5],xr[5]],[0,n2.pdf(xr[5])*w2],linestyle='--')\n",
    "plt.plot([xr[1],xr[1]],[0,n1.pdf(xr[1])*w1],color='blue',linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHmcDvaUAZiB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El modelo como combinación lineal de dos funciones base es el siguiente:\n",
    "\n",
    "$$\n",
    "y = f(x) = w_1 \\kappa(x;c_1,\\gamma_1) + w_2 \\kappa(x;c_2,\\gamma_2)\n",
    "$$\n",
    "\n",
    "Por lo tanto, el **algoritmo** que aprenda el modelo tiene que encontrar:\n",
    "- los centros ideales para fijar las funciones base: $c_1$, $c_2$\n",
    "- la anchura ideal para las funciones base: $\\gamma_1$ y $\\gamma_2$\n",
    "- los pesos de la combinación: $w_1$ y $w_2$\n",
    "\n",
    "Una vez más, nuestra política de *no spoiler* nos lleva a recurrir a la suerte y la fuerza bruta para encontrar valores razonables para estos parámetros. Con ello generaremos un conjunto de modelos *kernel* candidatos y nos quedaremos con el que explique mejor los datos, es decir, con el que arroje menor **error cuadrático** al modelar las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JF3fTc8xAZiE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class kernel_dos(object):\n",
    "    def __init__(self, c1, c2, gamma, w1, w2):\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.gamma = gamma\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "    def predict(self, x):\n",
    "        e1 = np.exp(-self.gamma*(self.c1 - x)**2)\n",
    "        e2 = np.exp(-self.gamma*(self.c2 - x)**2)\n",
    "        return w1*e1 + w2*e2\n",
    "    \n",
    "    def pinta(self,x,aa):\n",
    "        e1 = np.exp(-self.gamma*(self.c1 - x)**2)\n",
    "        e2 = np.exp(-self.gamma*(self.c2 - x)**2)\n",
    "        aa.fill_between(x,0,e1*self.w1,alpha=0.2)\n",
    "        aa.fill_between(x,0,e2*self.w2,alpha=0.2)\n",
    "        aa.plot([self.c1, self.c1],[0,self.w1],linestyle='--')\n",
    "        aa.plot([self.c2, self.c2],[0,self.w2],linestyle='--')\n",
    "        aa.plot(x,self.w1*e1 + self.w2*e2, linewidth=3, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mis0ZNsRAZiH"
   },
   "outputs": [],
   "source": [
    "n_candidatos = 20\n",
    "lista_kernel_regressor = []\n",
    "for cc in range(n_candidatos):\n",
    "    c1, c2 = np.random.randint(0,10, size=2)\n",
    "    \n",
    "    if c1 == c2:\n",
    "        c2 = c1+1\n",
    "        if c2 == 10:\n",
    "            c2=0\n",
    "    \n",
    "    gamma = np.random.uniform(1, 2, size=1)[0]\n",
    "    \n",
    "    w1, w2 = np.random.uniform(0.1, 1, size=2)\n",
    "    lista_kernel_regressor.append(kernel_dos(c1, c2, gamma, w1, w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14438,
     "status": "ok",
     "timestamp": 1592304273956,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "KxeaiIp8AZiN",
    "outputId": "de5229c3-5969-45fc-83a2-30cf72aa5051"
   },
   "outputs": [],
   "source": [
    "ff,aa = plt.subplots(4,5, sharex=True, sharey=True, figsize=(14,10))\n",
    "kk=0\n",
    "sq_error = np.empty(n_candidatos)\n",
    "for ss in range(4):\n",
    "    for cc in range(5):\n",
    "        aa[ss][cc].scatter(xr,yr)\n",
    "        lista_kernel_regressor[kk].pinta(x, aa[ss][cc])\n",
    "        pred_ = lista_kernel_regressor[kk].predict(xr)\n",
    "        sq_error[kk] = np.mean((pred_-yr)**2)\n",
    "        aa[ss][cc].set_title('err: {0:.4f}'.format(sq_error[kk]))\n",
    "        kk += 1\n",
    "ff.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14432,
     "status": "ok",
     "timestamp": 1592304273957,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "6kjS-K_SAZiR",
    "outputId": "84da1c75-8d82-4b28-8a0e-378f124b1b90",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "best_k = np.argmin(sq_error)\n",
    "print(\"El mejor regresor es el {0:d}\".format(best_k))\n",
    "print(\"centros: {0:.2f}, {1:.2f}\".format(lista_kernel_regressor[best_k].c1, lista_kernel_regressor[best_k].c2))\n",
    "print(\"gamma: {0:.2f}\".format(lista_kernel_regressor[best_k].gamma))\n",
    "print(\"pesos: {0:.2f}, {1:.2f}\".format(lista_kernel_regressor[best_k].w1, lista_kernel_regressor[best_k].w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNuLt6k2AZiV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En general un método de *kernels* elige cuidadosamente dentro de una optimización más o menos costosa **cuántas funciones base** son necesarias, la anchura de las mismas y los pesos de la combinación.\n",
    "\n",
    "Pero es una de las maneras más **robustas** de construir modelos no lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Intro_aprendizaje_automatico.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
